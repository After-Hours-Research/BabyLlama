# Baby Llama From Scratch
This is an implementation of Llama from scratch. It includes writing our own attention mechanism that include the three main components of the Llama paper:
1. RMSNorm for pre-normalization
2. RoPE (Rotary Positional Embedding)
3. SwiGLU activation function

Here is a diagram illustrating a single block:
![](/imgs/diagram1.png)

